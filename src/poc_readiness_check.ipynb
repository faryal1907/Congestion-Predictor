{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ceec930-d57c-41c6-a4db-7e458c6f2b66",
   "metadata": {},
   "source": [
    "# **PoC Readiness Check: Generate Sample & Test Predict Function**\n",
    "**Run after:** model_pipeline.ipynb.  \n",
    "**Outputs:** sample_for_poc.csv (100 rows from eda_merged.csv for dashboard).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea42436-abf1-4d3d-9476-06955f09bdd6",
   "metadata": {},
   "source": [
    "## **Imports & Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34358954-3043-40d7-b30f-db35695bf70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PoC Readiness Started ===\n",
      "Processed: ../data/processed/ (eda_merged.csv expected)\n",
      "Models: ../models/ (XGBoost_Tuned.pkl expected)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Paths (relative to root from src/)\n",
    "processed_path = \"../data/processed/\"\n",
    "models_path = \"../models/\"\n",
    "\n",
    "print(\"=== PoC Readiness Started ===\")\n",
    "print(f\"Processed: {processed_path} (eda_merged.csv expected)\")\n",
    "print(f\"Models: {models_path} (XGBoost_Tuned.pkl expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750c313-26e7-4e9d-8b76-121bc6b22106",
   "metadata": {},
   "source": [
    "## **Load Merged Data & Generate Sample**\n",
    "- Load eda_merged.csv, take sample\n",
    "- Save: sample_for_poc.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a1871f-f5dd-42fd-8743-23ad83fcd2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Load & Generate Sample ===\n",
      "Eda_merged shape: (125536, 24)\n",
      "Low group size: 31936, sampled: 25\n",
      "Medium group size: 62216, sampled: 50\n",
      "High group size: 31384, sampled: 25\n",
      "Sample shape: (100, 24) (balanced: 25 low, 50 medium, 25 high)\n",
      "Sample score stats:\n",
      " count    100.00\n",
      "mean       1.70\n",
      "std        3.42\n",
      "min        0.00\n",
      "25%        0.00\n",
      "50%        0.00\n",
      "75%        1.01\n",
      "max       10.00\n",
      "Name: congestion_score_clipped, dtype: float64\n",
      "Sample incident stats:\n",
      " incident_count\n",
      "0    72\n",
      "1    23\n",
      "2     4\n",
      "3     1\n",
      "Name: count, dtype: int64\n",
      "Sample head (diverse):\n",
      "              datetime  incident_count   Latitude  Longitude  segment_id   dist_km    length        highway  mobility_proxy       tavg       prcp   humidity  latitude  longitude  temperature  precipitation  precip_lag    length_m  congestion_score  hour  dayofweek  congestion_score_clipped  peak_hour  weekday\n",
      "0 2020-02-29 05:00:00               0  31.561020  74.357043       16430  0.001229  0.736244    residential             2.0  17.241667  17.029167  83.166667   31.5497    74.3436    17.241667      17.029167   17.242708  736.244098          0.002040     5          5                  0.002040          0        0\n",
      "1 2020-03-15 09:45:00               0  31.554368  74.340897        2097  0.000393  0.402761    residential             4.0  15.990625   0.000000  77.781250   31.5497    74.3436    15.990625       0.000000    0.000000  402.761041          0.001040     9          6                  0.001040          0        0\n",
      "2 2020-03-20 01:30:00               0  31.528032  74.341635       11221  0.000224  0.176748  living_street           -16.0  19.793750   1.337500  76.000000   31.5497    74.3436    19.793750       1.337500    1.347917  176.747617          0.001936     1          4                  0.001936          0        1\n",
      "✓ Saved balanced sample_for_poc.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Load & Generate Sample ===\")\n",
    "eda_merged = pd.read_csv(processed_path + \"eda_merged.csv\")\n",
    "eda_merged['datetime'] = pd.to_datetime(eda_merged['datetime'])  # Ensure datetime\n",
    "\n",
    "# Balanced sample (stratify by score quartiles for diversity, with safe sample size)\n",
    "q25 = eda_merged['congestion_score_clipped'].quantile(0.25)\n",
    "q75 = eda_merged['congestion_score_clipped'].quantile(0.75)\n",
    "\n",
    "low_df = eda_merged[eda_merged['congestion_score_clipped'] <= q25]\n",
    "medium_df = eda_merged[(eda_merged['congestion_score_clipped'] > q25) & (eda_merged['congestion_score_clipped'] <= q75)]\n",
    "high_df = eda_merged[eda_merged['congestion_score_clipped'] > q75]\n",
    "\n",
    "# Safe sample: min(requested, available) to avoid ValueError\n",
    "n_low = min(25, len(low_df))\n",
    "n_medium = min(50, len(medium_df))\n",
    "n_high = min(25, len(high_df))\n",
    "\n",
    "low = low_df.sample(n_low, random_state=42) if n_low > 0 else low_df.head(n_low)\n",
    "medium = medium_df.sample(n_medium, random_state=42) if n_medium > 0 else medium_df.head(n_medium)\n",
    "high = high_df.sample(n_high, random_state=42) if n_high > 0 else high_df.head(n_high)\n",
    "\n",
    "sample = pd.concat([low, medium, high]).sort_values('datetime').reset_index(drop=True)  # Sort for time demo\n",
    "\n",
    "sample.to_csv(processed_path + \"sample_for_poc.csv\", index=False)\n",
    "\n",
    "print(f\"Eda_merged shape: {eda_merged.shape}\")\n",
    "print(f\"Low group size: {len(low_df)}, sampled: {n_low}\")\n",
    "print(f\"Medium group size: {len(medium_df)}, sampled: {n_medium}\")\n",
    "print(f\"High group size: {len(high_df)}, sampled: {n_high}\")\n",
    "print(f\"Sample shape: {sample.shape} (balanced: {n_low} low, {n_medium} medium, {n_high} high)\")\n",
    "print(\"Sample score stats:\\n\", sample['congestion_score_clipped'].describe().round(2))\n",
    "print(\"Sample incident stats:\\n\", sample['incident_count'].value_counts().sort_index())\n",
    "print(\"Sample head (diverse):\\n\", sample.head(3).to_string())\n",
    "\n",
    "print(\"✓ Saved balanced sample_for_poc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92248d0-8da5-4e0a-b65e-80edddc00a7c",
   "metadata": {},
   "source": [
    "## **Predict Function & Test**\n",
    "- Load tuned XGBoost (fallback untuned/RF).\n",
    "- Function: Validates inputs (non-neg), predicts, clips 0-10.\n",
    "- Test: Base/low (1 incident, no rain) vs high (5 incidents, rain, peak)—expect ~2.7 vs ~10.0.\n",
    "- Handles errors (missing model, bad input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1324a61e-4480-4483-aad6-a23616627798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predict Function & Test ===\n",
      "Loaded XGBoost Tuned.\n",
      "Predict function tested & ready for PoC!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Predict Function & Test ===\")\n",
    "\n",
    "# Robust model load\n",
    "try:\n",
    "    model = joblib.load(models_path + \"XGBoost_Tuned.pkl\")\n",
    "    print(\"Loaded XGBoost Tuned.\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        model = joblib.load(models_path + \"XGBoost_Untuned.pkl\")\n",
    "        print(\"Fallback to XGBoost Untuned.\")\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            model = joblib.load(models_path + \"Random_Forest_Untuned.pkl\")\n",
    "            print(\"Fallback to RF Untuned.\")\n",
    "        except:\n",
    "            print(\"Error: No model found. Run model_pipeline.ipynb.\")\n",
    "            model = None\n",
    "\n",
    "# Predict function with validation\n",
    "def predict_congestion(incident, precip, humidity, tavg, peak_hour, weekday):\n",
    "    if model is None:\n",
    "        return None\n",
    "    try:\n",
    "        if precip < 0 or incident < 0 or humidity < 0 or tavg < -50:\n",
    "            print(\"Warning: Invalid input (non-neg precip/incident; realistic humidity/temp).\")\n",
    "            return None\n",
    "        feature_df = pd.DataFrame({\n",
    "            'incident_count': [incident],\n",
    "            'precipitation': [precip],\n",
    "            'humidity': [humidity],\n",
    "            'tavg': [tavg],\n",
    "            'peak_hour': [peak_hour],\n",
    "            'weekday': [weekday]\n",
    "        })\n",
    "        raw_score = model.predict(feature_df)[0]\n",
    "        score = np.clip(raw_score, 0, 10)  # Clip for interpretability\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Predict function tested & ready for PoC!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535e6f6a-b643-4538-a610-0e8c9119c4d5",
   "metadata": {},
   "source": [
    "## **PoC Readiness Summary**\n",
    "- **sample_for_poc.csv**: 100 rows (eda_merged head, datetime/incident/score).\n",
    "- **Predict Function**: Validates inputs, loads model (fallback), clips score 0-10.\n",
    "- **Next**: Run `streamlit run poc/poc.py` (loads sample/model, interactive tabs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
